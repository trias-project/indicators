---
title: "Study of research effort bias"
author:
  - Damiano Oldoni
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---
  
# Goal 

This document is an ongoing research trying to find an effective method of compensating the *research effort bias*. We will try it on data of occurrences (number of occurrences per year in Belgium) and occupancy (number of 1x1km cells occupied per year in Belgium). 

In particular, this document focuses on the aggregation technique. 

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Load libraries:
  
```{r load_libraries}
library(rgbif)        # GBIF interface
library(tidyverse)    # Data science
library(sf)           # GeoSpatial operations
library(multidplyr)   # Parallel computing
library(leaflet)      # Create maps
library(zoo)          # time series
```

# Define species under study

We are interested in the following alien species:
  
species | kingdom | GBIF backbone key
--- | --- | ---
Baccharis halimifolia | Plantae | 3129663
Impatiens glandulifera | Plantae | 2891770
Impatiens capensis | Plantae | 2891774
Hydrocotyle ranunculoides | Plantae | 7978544
Branta canadensis | Animalia | 5232437
Harmonia axyridis | Animalia | 4989904

limited to Belgium (`countryCode = BE`).

# Import data 

## Import occurrence data

A GBIF download has been triggered with this download key: `"0012078-181003121212138"`. Import data:

```{r get_dwc_file_from GBIF}
gbif_key <- "0012078-181003121212138"
occ_df_file <- paste0("../data/interim/occ_df_", 
                                          gbif_key, ".csv")
# downloaded zip file is in ./data/interim/
file_path <- paste0("../data/interim/")
if (!file.exists(file_path))
  dir.create(file_path)
if (!file.exists(occ_df_file)){
  occ <- occ_download_get(key = gbif_key, overwrite = TRUE, path = file_path)
fn <- "occurrence.txt"
unzip(zipfile = occ, files = fn,
      exdir = "../data/interim/.")
file.rename(from = "../data/interim/occurrence.txt", 
            to = occ_df_file)
}
```

Import text file in R session:
  
```{r import_to_R_as_df}
occ_df <- read_delim(
  file = occ_df_file, delim = "\t",
  escape_double = FALSE, trim_ws = TRUE)
```

Remove eventually misleading parsing failures:

```{r remove_parsing_failures}
occ_df <- occ_df %>%
   filter(species != "2986")
```

Species:

```{r species}
species_df <- occ_df %>%
  distinct(species, speciesKey)
species_df
```

Number of occurrences per species:
  
```{r n_occ_per_species}
count_occ_df <- occ_df %>%
  group_by(speciesKey, species) %>%
  count() %>%
  arrange(desc(n))
count_occ_df
```

## Pre-process occurrence data

### basisOfRecord

Everything except `FOSSIL_SPECIMEN` (is historical distribution) and `LIVING_SPECIMEN` (includes location of captured taxa).

```{r show_specimen}
occ_df %>%
  group_by(basisOfRecord) %>%
  count()
```

We use only human observations:

```{r occs_only_human_obs}
occ_df <- 
  occ_df %>%
  filter(!basisOfRecord %in% c("FOSSIL_SPECIMEN", "LIVING_SPECIMEN"))
```

## Remove occurrence related to absence

Some occurrences have `occurrenceStatus` equal to `absent` and `excluded`:

```{r show_summary_occtarrenceStatus}
occ_df %>%
  group_by(occurrenceStatus) %>%
  count()
```

We will filter out occurrence points related to following species:

```{r summary_species_absency}
occ_df %>%
  filter(occurrenceStatus %in% c("absent", "excluded")) %>%
  group_by(speciesKey, species, occurrenceStatus) %>%
  count()
```

We remove them:

```{r remove_absence}
occ_df <- occ_df %>%
  filter(!occurrenceStatus %in% c("absent", "excluded"))
```


## Check temporal information

Occurrence data without temporal information cannot be handled. Overview:

```{r temporal_info}
occ_df %>%
  filter(is.na(year)) %>%
  group_by(speciesKey, species, datasetKey, datasetName) %>%
  count()
```

We remove them:

```{r remove_occ_without_time}
occ_df <- occ_df %>%
  filter(!is.na(year))
```

### Check presence of coordinates

Occurrences without coordinates:

```{r n_occs_with_no_coords}
occ_df %>%
  filter(is.na(decimalLatitude) | is.na(decimalLongitude)) %>%
  group_by(speciesKey, species) %>%
  count()
```

We remove them:

```{r filter_valid_geo_coords}
occ_df <- occ_df %>%
  filter(!is.na(decimalLatitude) & !is.na(decimalLongitude))
```

Note: this filter is equivalent of a GBIF download query containing the additional parameter `hasCoordinate = TRUE`.

Number of occurrences per species left:
  
```{r n_occ_per_species_with_coord}
count_occ_df <- occ_df %>%
  group_by(speciesKey, species) %>%
  count()
count_occ_df
```

### Handle data without spatial uncertainty

Uncertainty on position is given in `coordinateUncertaintyInMeters` while in `verbatimCoordinateSystem` we can get details about the coordinate system used while collecting data:

```{r overview_unc_verbatimCRS}
occ_df %>% 
  group_by(coordinateUncertaintyInMeters, verbatimCoordinateSystem) %>%
  count() %>%
  arrange(desc(n))
```

Overview of occurrence data with missing coordinate uncertainty:

```{r details_data_with_no_coordinate_uncertainty}
occ_df %>% 
  filter(is.na(coordinateUncertaintyInMeters)) %>%
  group_by(datasetKey, datasetName, speciesKey, species) %>%
  count() %>%
  arrange(desc(n)) %>%
  ungroup() %>%
  select(datasetName, species, n, everything())
```

In some cases, based on the dataset and the coordinate system information, we can assign the right uncertainty. However, this approach is not machine-based, very time consuming and not feasable at large scales. A possible solution is the following: **if missing, we assign a default uncertainty of 1000 meters**.

```{r assign_default_1000m_uncertainty}
occ_df <- occ_df %>%
  mutate(coordinateUncertaintyInMeters = ifelse(
    is.na(coordinateUncertaintyInMeters), 1000.0, coordinateUncertaintyInMeters)
  )
```

Overview of the most common coordinate uncertainty for each species and year, the _mode_:

```{r mode_coord_unc}
calc_mode <- function(data) {
  data %>% 
    group_by(coordinateUncertaintyInMeters) %>%
    count() %>%
    arrange(desc(n)) %>%
    ungroup() %>%
    slice(1) %>%
    pull(coordinateUncertaintyInMeters)
}

mode_unc_df <- occ_df %>%
  group_by(species, speciesKey, year) %>%
  nest() %>%
  mutate(mode_uncertainty = map_dbl(data, calc_mode)) %>%
  select(-data) %>%
  arrange(species, year)
mode_unc_df
```

## Import grid data

We import UTM grid data at 1 by 1 km resolution. Grid has CRS (Coordinate Reference System) _Belge Lambert 1972_:

```{r import_grids_of_BE_from_EEA}
belgium_grid <- st_read("../data/external/utm1_bel/be_1km.shp")
crs_grid <- st_crs(belgium_grid)
crs_grid
```

Total area of the grid in square meters:

```{r get_total_area}
total_area <- as.numeric(sum(st_area(belgium_grid)))
total_area
```

and mean cell size in meters:

```{r mean_cell_size}
cell_size <- as.numeric(mean(st_area(belgium_grid)))
cell_size
```

# Calculate Occupancy by point-in-circle technique

Notice that GBIF occurrence data use WGS84 projection:

```{r set_crs_GBIF_data}
# CRS used by GBIF
wgs_84 <- st_crs("+init=epsg:4326") 
```

Create points and convert them to grid projection:

```{r from_df_to_points}
occ_pts <- st_as_sf(occ_df, coords = c('decimalLongitude', 'decimalLatitude'), 
                    crs = wgs_84)
occ_pts <- st_transform(occ_pts, crs_grid)
```

We use uncertainty in `coordinateUncertaintyInMeters` to create circles linked to occurrences:

```{r create_circles}
occ_circles <- st_buffer(occ_pts, 
                         dist = occ_pts$coordinateUncertaintyInMeters,
                         nQuadSegs = 6)
```

Some occurrence data are quite coarse and geographically aggregated: they are assigned to same coordinates, thus resulting in darker points on map. A way to solve this issue and at the same time to take into account the coordinate uncertainty, we assign the occurrence data to a point randomly chosen within the circle. Note: this step can take long:

```{r assign_pts_randomly_within_circle, cache=TRUE}
get_random_pt <- function(x) {
  random_pt <- st_sample(x , size = 1, type = "random")
  while (length(random_pt) == 0) {
    random_pt <- st_sample(x , size = 1, type = "random")
  }
  return(random_pt)
}

random_pts <- map(st_geometry(occ_circles), get_random_pt)
random_pts <- st_sfc(unlist(random_pts, recursive = FALSE), crs = crs_grid)
```

Assign these points to the occurrence data:

```{r assign_pts_to_circles_randomize}
occ_pts_randomize <- st_sf(data.frame(occ_df, geom = random_pts))
```

Preview on a map:

```{r check_randomize}
# add preview (select a subset of polygons via st_intersect)
n_pts <- 50
preview_pts_circles <- occ_pts_randomize[1:n_pts,]
preview_circles <- occ_circles[1:n_pts,]
subset_grid <- st_intersection(belgium_grid, preview_circles)$CELLCODE
subset_grid <- belgium_grid %>%
  filter(CELLCODE %in% subset_grid)
mapt <- leaflet() %>%
  addTiles() %>%
  addPolygons(data = subset_grid %>%
                st_transform(wgs_84),
              weight = 1, opacity = 0.5, fillOpacity = 0.1) %>%
  addCircleMarkers(data = preview_pts_circles %>%
                     st_transform(wgs_84), 
                   radius = 2, color = "red") %>%
  addPolygons(data = preview_circles %>%
                st_transform(wgs_84),
              weight = 1, opacity = 0.3, fillOpacity = 0.3)
  
mapt
```

Intersect points and grid:

```{r intersect, cache=TRUE}
points_on_grid <- st_intersection(belgium_grid, occ_pts_randomize)
```

Only grid cells containing at least one point are present. 

We calculate the number of occurrences per cell and assign 1 (presence) to them. This step can take long due to the high number of cells of the grid with 1x1km resolution:

```{r pres_abs, cache=TRUE}
pres_table <- partition(points_on_grid, CELLCODE) %>%
  group_by(speciesKey, year) %>%
  summarise(dplyr::n()) %>%
  as.tibble(pres_year) %>%
  ungroup() %>%
  rename("n_occs" = "dplyr::n()") %>%
  filter(n_occs > 0) %>%
  mutate(presence = 1) %>%
  left_join(species_df, by = "speciesKey") %>%
  select(speciesKey, species, everything())
```

For each species and year, we calculate the Area Of Occupancy (AOO) and the Occupancy (AOO divided by total grid area):

```{r AOO_calculate}
aoo <- 
  pres_table %>%
  group_by(species, speciesKey, year) %>% 
  summarize(aoo = n()*cell_size, n_occs = sum(n_occs)) %>%
  ungroup() %>%
  mutate(occupancy = aoo / total_area,
         cell_area = cell_size
)
```

AOO plot:

```{r AOO_plot}
ggplot(aoo) + 
  aes(x = year, y = aoo/10^6) +
  facet_wrap(~ species) +
  geom_col() +
  labs(x = "year", y = "AOO (km2)") +
  ggtitle(names(aoo))
```

Occupancy plot:

```{r occupancy_plot}
ggplot(aoo) + 
  aes(x = year, y = occupancy) +
  facet_wrap(~ species) +
  geom_col() +
  labs(x = "year", y = "occupancy") +
  ggtitle(names(aoo))
```

# Research effort bias

The difference in occupancy along the years contains a bias which is typically called *research effort bias*. The presence of some species is not monitored constantly each year with the same effort and it can happen that a campaign runs over the entire country in different years. Moreover, the observations coming from citizen science projects are typically scattered over time and place and their contribute is difficult to estimate a priori.  Such variability risks to covers variations in the spreading of aliean species. In TriAS we will have to work with thousands of species, so we need to develop a technique which is not only effective but also as much general as possible.

## Aggregation technique

In this section we study how aggregating data on time can partly remove the effort bias.

We calculate the occupancy levels by using different aggregation spans, from 1 (no aggregation) up to 10 years:

```{r t_span}
t_span <- c(1:10)
```

Preprocessing: introduce years where no occupancy is detected and set `presence = 0`:

```{r add_absences}
# Get all cells occupied at least once for each species
cellcodes_species <- 
  pres_table %>%
  distinct(speciesKey, species, CELLCODE)
# Get series years for each species and join on the cells
years_species_cell <- 
  pres_table %>%
  distinct(speciesKey, species, year) %>%
  group_by(speciesKey, species) %>%
  complete(year = seq(min(year, na.rm = TRUE), 
                      max(year, na.rm = TRUE))) %>%
  left_join(cellcodes_species, by = c("speciesKey", "species"))

pres_abs_table <- 
  pres_table %>%
  right_join(years_species_cell, 
            by = c("speciesKey", "species", "year", "CELLCODE")) %>%
  mutate(presence = if_else(!is.na(presence), presence, 0)) %>%
  select(-n_occs)
```

Apply moving sum

```{r running_aggregation, cache=TRUE}
progress_bar <- progress_estimated(length(t_span))
pres_abs_table_mov_sum <- 
  map_dfr(
    t_span, function(t) {
      progress_bar$tick()$print()
      pres_abs_table %>%
        group_by(speciesKey, species, CELLCODE) %>%
        mutate(mov_sum = c(
          rep(0, t - 1), 
          rollsum(na.fill(presence, 0), t, align = "right"))) %>%
        ungroup() %>%
        mutate(presence = if_else(mov_sum > 0, 1, 0)) %>%
        mutate(t_window = t)
    }
)
```

Calculate the AOO for each time window:

```{r aggregated_AOO}
aoo_mov_sum <- 
  pres_abs_table_mov_sum %>%
  group_by(species, speciesKey, year, t_window) %>% 
  summarize(aoo = sum(presence)*cell_size) %>%
  ungroup() %>%
  mutate(occupancy = aoo / total_area)
```

Occupancy is a monotonous function of the time window use for calculating the moving sum. Some examples:

```{r plot_aggregated_AOO}
map(c(2008:2012), function(x) {
  ggplot(aoo_mov_sum %>% filter(year == x),
         aes(x = t_window, y = occupancy, color = species)) +
    geom_point() + 
    geom_line() +
    scale_x_continuous(breaks = c(1:10))
})
```

Please notice that a time window of 1 is equivalent of not applying any aggregation. The values are right aligned: as example, the occupancy calculated at year 2010 and time window 10 is the sum of occupancy from 2001 to 2010.

If we set the occupancy at time window 10  as our reference unit, we can calculate summary statistics: how many years do we have to aggregate to get a relative occupancy higher than or equal to 0.1, 0.2, ..., 1.0?

```{r percentiles_distr}
thresholds <- seq(0.1, 1.0, by = 0.1)
rel_occ_perc <- 
  map_dfr(thresholds, 
          function(perc) {
            aoo_mov_sum %>%
              group_by(species, speciesKey, year) %>%
              # occupancy at longest time window is the maximum value possible
              mutate(rel_occ = occupancy/max(occupancy)) %>%
              filter(!is.na(rel_occ)) %>%
              filter(rel_occ >= perc) %>%
              filter(rel_occ == min(rel_occ)) %>%
              filter(t_window == min(t_window)) %>%
              ungroup() %>%
              select(speciesKey, species, year, t_window, rel_occ) %>%
              mutate(threshold = perc)
          }
)
rel_occ_perc
```

We calculate the distributions of the time windows for each threshold:

```{r plot_distribution_t}
map(thresholds, function(perc) {
  ggplot(rel_occ_perc %>% filter(threshold == perc),
         aes(x = t_window, fill = species)) +
    geom_bar(stat = "count") +
    scale_x_continuous(breaks = c(1:10)) +
    ggtitle(paste0("Relative occupancy >=", perc*100, "% of occupancy at 10 year time window."))
})
```

